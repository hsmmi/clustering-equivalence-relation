{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Project 1 on Fuzzy system</center>\n",
    "\n",
    "Name: Hesam Mousavi\n",
    "\n",
    "Student number: 9931155\n",
    "\n",
    "Master student\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    "MathJax.Hub.Config({\n",
    "tex2jax: {\n",
    "inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n",
    "processEscapes: true},\n",
    "jax: [\"input/TeX\",\"input/MathML\",\"input/AsciiMath\",\"output/CommonHTML\"],\n",
    "extensions: [\"tex2jax.js\",\"mml2jax.js\",\"asciimath2jax.js\",\"MathMenu.js\",\"MathZoom.js\",\"AssistiveMML.js\", \"[Contrib]/a11y/accessibility-menu.js\"],\n",
    "TeX: {\n",
    "extensions: [\"AMSmath.js\",\"AMSsymbols.js\",\"noErrors.js\",\"noUndefined.js\"],\n",
    "equationNumbers: {\n",
    "autoNumber: \"AMS\"\n",
    "}\n",
    "}\n",
    "});\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from my_io import read_dataset_to_X_and_y\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Class to easily have all the variables\n",
    "\n",
    "I like to have my variable all together so I build a class and named it\n",
    "UniSet(short form of universal set)\n",
    "\n",
    "Read dataset with my function on my_io module that can shuffle sample and\n",
    "correct missing values also normalized the feature.  \n",
    "\n",
    "In here I shuffle data and use class-mean for the missing values then\n",
    "normalized it with the z-score method(zero-mean unit-variance)\n",
    "\n",
    "I use all the features(12) and change sex from m, f to 0, 1 (actually I map\n",
    "each string to a specific number in my_io module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole dataset is (615, 12) matrix\n"
     ]
    }
   ],
   "source": [
    "class UniSet():\n",
    "    def __init__(self, file, range_feature, range_label,\n",
    "                 normalization=None, shuffle=False, about_nan='class_mean'):\n",
    "        np.random.seed(1)\n",
    "        sample, label = read_dataset_to_X_and_y(\n",
    "            file, range_feature, range_label, normalization, shuffle=shuffle,\n",
    "            about_nan=about_nan)\n",
    "        self.universal = sample.astype(float)\n",
    "        self.label = label\n",
    "        self.number_of_feature = sample.shape[1]\n",
    "        self.size_of_universal = sample.shape[0]\n",
    "        self.diffrent_label = np.unique(label)\n",
    "        self.number_of_diffrent_label = self.diffrent_label.shape[0]\n",
    "        self.relation = None\n",
    "        self.equivalence_relation = None\n",
    "\n",
    "\n",
    "uni_total = UniSet(\n",
    "    'dataset/hcvdat0.csv', (2, 14), (1, 2),\n",
    "    normalization='z_score', shuffle=True, about_nan='class_mean')\n",
    "\n",
    "\n",
    "print(f'The whole dataset is {uni_total.universal.shape} matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Details \n",
    ">\n",
    "> In my_io module I have a function named read_dataset_to_X_and_y\n",
    ">  that get dataset file, range of attributes that are our features,\n",
    ">  range of attributes that are our labels, normalization which is\n",
    ">  our normalization method, shuffle which if be True our samples be\n",
    ">  shuffled, and about_nan that can be \"delete\" which delete samples\n",
    ">  with NA values or \"class_mean\" which replace NA values with mean of\n",
    ">  that feature in the sample class\n",
    ">\n",
    "> Also as I mentioned above this function can get string attributes too\n",
    ">  by mapping each string to a specific value so now our labels $\\in [0, 4]$\n",
    ">\n",
    "> I change NA value with class-mean because It doesn't change the\n",
    ">  similarity(or distance) of two samples in one class \n",
    ">\n",
    "> In my class, I have all things that I'll need such as universal\n",
    ">  (sample data), their label, number of features, size of universal\n",
    ">  (dataset), different labels (unique labels), number of different labels,\n",
    ">  a relation (on our universal set), and an equivalence relation.\n",
    ">\n",
    "> Our labels in this dataset is attributed [1, 2) and features are attributed\n",
    ">  [2, 14) (12 features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the whole dataset to Train and Test\n",
    "\n",
    "As I shuffle the dataset before, now I just consider the first 80%\n",
    "of the data for the train and the rest for the test case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset is (492, 12) matrix\n",
      "The test dataset is (123, 12) matrix\n"
     ]
    }
   ],
   "source": [
    "def split_train_test(universe: UniSet, train_size: float) -> list[UniSet]:\n",
    "    train = deepcopy(universe)\n",
    "    test = deepcopy(universe)\n",
    "    train.size_of_universal = \\\n",
    "        int(universe.size_of_universal*train_size)\n",
    "    train.universal = \\\n",
    "        universe.universal[0:train.size_of_universal]\n",
    "    train.label = \\\n",
    "        universe.label[0:train.size_of_universal]\n",
    "    test.size_of_universal = (\n",
    "        universe.size_of_universal - train.size_of_universal)\n",
    "    test.universal = \\\n",
    "        universe.universal[train.size_of_universal:]\n",
    "    test.label = \\\n",
    "        universe.label[train.size_of_universal:]\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "uni_train, uni_test = split_train_test(uni_total, 0.8)\n",
    "print(f'The train dataset is {uni_train.universal.shape} matrix')\n",
    "print(f'The test dataset is {uni_test.universal.shape} matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Details \n",
    ">\n",
    "> I create two classes for train and test by copying the total set and\n",
    "just changing universal, level, and size of universal for both train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find relation matrix\n",
    "\n",
    "To find the relation between each sample, first, we need metric for\n",
    "similarity\n",
    "\n",
    "### Similarity metric\n",
    "\n",
    "First I find the distance between each pair of samples and, then\n",
    "normalized it by dividing them to maximum distance, now all the\n",
    "distances are between [0, 1] and now similarity is just equal to\n",
    "1 - distance\n",
    "\n",
    "$$\n",
    "R\\left(\\mathbf{x}_{i}, \\mathbf{x}_{k}\\right)=1-\\delta\\left\n",
    "(\\sum_{j=1}^{p}\\left|x_{i j}-x_{k j}\\right|^{q}\\right)^{\\frac{1}{q}}\n",
    "$$\n",
    "which $\\delta$ is maximum distance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relation on train dataset is (492, 492) matrix\n",
      "The relation on test dataset is (123, 123) matrix\n"
     ]
    }
   ],
   "source": [
    "def distance(sample1: np.ndarray, sample2: np.ndarray) -> float:\n",
    "    return np.linalg.norm(sample1-sample2)\n",
    "\n",
    "\n",
    "def find_relation(universal: UniSet) -> np.ndarray:\n",
    "    dis = np.array(\n",
    "        list(map(lambda x: list(map(\n",
    "            lambda y: distance(\n",
    "                universal.universal[x], universal.universal[y]),\n",
    "            range(universal.size_of_universal))),\n",
    "            range(universal.size_of_universal))))\n",
    "    return 1 - dis / np.max(dis)\n",
    "\n",
    "\n",
    "uni_train.relation = find_relation(uni_train)\n",
    "print(f'The relation on train dataset is {uni_train.relation.shape} matrix')\n",
    "\n",
    "\n",
    "uni_test.relation = find_relation(uni_test)\n",
    "print(f'The relation on test dataset is {uni_test.relation.shape} matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Details \n",
    ">\n",
    "> Our Similarity metric has some properties:\n",
    ">\n",
    "> Similarity $(x, y) \\in[0,1]$\n",
    ">\n",
    "> Similarity $(x, x)=1$\n",
    ">\n",
    "> Similarity $(x, y)=$ Similarity $(y, x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make a relation transitive?\n",
    "\n",
    "I use the following algorithm to find a transitive closure of a relation.\n",
    "\n",
    "1. $R^{\\prime}=R \\cup(R \\circ R)$.\n",
    "2. If $R^{\\prime} \\neq R$, make $R=R^{\\prime}$ and go to Step 1 .\n",
    "3. Stop: $R^{\\prime}=R_{T}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Iteration to make train relation transitive\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "The equivalence relation on train dataset is (492, 492) matrix\n",
      "\n",
      "#Iteration to make test relation transitive\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "The equivalence relation on test dataset is (123, 123) matrix\n"
     ]
    }
   ],
   "source": [
    "def max_min(sample1: np.ndarray, sample2: np.ndarray) -> float:\n",
    "    both_sample = np.vstack((sample1, sample2))\n",
    "    return np.max(np.min(both_sample, axis=0))\n",
    "\n",
    "\n",
    "def composition_RoR(relation: np.ndarray) -> np.ndarray:\n",
    "    result = np.array(\n",
    "        list(map(lambda x: list(map(\n",
    "            lambda y: max_min(relation[x], relation[y]),\n",
    "            range(relation.shape[0]))),\n",
    "            range(relation.shape[0]))))\n",
    "    return result\n",
    "\n",
    "\n",
    "def union_two_relation(\n",
    "        relation1: np.ndarray, relation2: np.ndarray) -> np.ndarray:\n",
    "    both_relation = np.dstack((relation1, relation2))\n",
    "    return np.max(both_relation, axis=2)\n",
    "\n",
    "\n",
    "def make_transitive(relation: np.ndarray) -> np.ndarray:\n",
    "    R = None\n",
    "    Rp = np.copy(relation)\n",
    "    iter = 0\n",
    "    while((Rp != R).any()):\n",
    "        R = np.copy(Rp)\n",
    "        RoR = composition_RoR(R)\n",
    "        Rp = union_two_relation(R, RoR)\n",
    "        iter += 1\n",
    "        print(iter)\n",
    "    return Rp\n",
    "\n",
    "\n",
    "print('#Iteration to make train relation transitive')\n",
    "uni_train.equivalence_relation = make_transitive(uni_train.relation)\n",
    "print('The equivalence relation on train dataset is',\n",
    "        f'{uni_train.equivalence_relation.shape} matrix')\n",
    "\n",
    "print('\\n#Iteration to make test relation transitive')\n",
    "uni_test.equivalence_relation = make_transitive(uni_test.relation)\n",
    "print('The equivalence relation on test dataset is',\n",
    "        f'{uni_test.equivalence_relation.shape} matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Details \n",
    ">\n",
    "> Order of max-min is $O\\left(n\\right)$ and in composition R and R\n",
    ">  we compose each pair of sample ($O\\left(n^{2}\\right)$) with max-min\n",
    ">  so function composition_RoR has order $O\\left(n^{3}\\right)$. To\n",
    ">  make relation transitive we need to compose our result \n",
    "> $\\lg \\left(\\left\\lceil\\frac{n}{2}\\right\\rceil\\right)$ to find transitive \n",
    "> relation along the longest path so out algotrithm is $O\\left(n^{3} \\lg n\\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalence relation\n",
    "\n",
    "Before find similar classes, we must find equivalence relation\n",
    "(be reflexive, symmetric, and transetive) to be sure If\n",
    "$R(x, y) \\geqslant \\alpha$ and\n",
    "$R(y, z) \\geqslant \\alpha \\Rightarrow R(x, z) \\geqslant \\alpha$ \n",
    "which cause all pairs in similarity class have similarity\n",
    "$\\geqslant \\alpha$ (transitive) and $R(x, y) = R(y, x)$ to be sure\n",
    "if x and y is in same class y and x can be in same class (symmetric)\n",
    "and $R(x, y) = 1$ to be sure x and x can be in a same similarity class (reflexive).\n",
    "\n",
    "By having all three property we sure that every element in a\n",
    "specific similarity class (cluster) have the same similarity class.\n",
    "\n",
    "### Reflexive\n",
    "\n",
    "Base on our similatiry metric, $similarity(x, x) = 1$ because\n",
    "distance x and x is zero and zero devided by any number is still\n",
    "zero and finally $1 - 0 = 1$ and for checking that our relation\n",
    "is reflexive we only need to check if $diag(R) = \\vec1$\n",
    "\n",
    "### Symmetric\n",
    "\n",
    "Base on our similatiry metric, $similarity(x, y) = similarity(y, x)$\n",
    "because only variable in our metric is distance and by using lebesgue\n",
    "norm, distance x from y is equal to distance y from x\n",
    "$\\Rightarrow similarity(x, y) = similarity(y, x)$ and for checking that\n",
    "our relation is symmetric we only need to check if $R = R^\\intercal$\n",
    "\n",
    "### Transitive\n",
    "\n",
    "With the above algorithm (in the previous cell) we make our relation\n",
    "transitive and for checking that our relation is transitive we only\n",
    "need to check if $R=R \\cup(R \\circ R)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is our train equivalence relation, equivalence? True\n",
      "Is our test equivalence relation, equivalence? True\n"
     ]
    }
   ],
   "source": [
    "def is_reflexive(relation: np.ndarray) -> bool:\n",
    "    return (relation.diagonal() == 1).all()\n",
    "\n",
    "\n",
    "def is_symmetric(relation: np.ndarray) -> bool:\n",
    "    return (relation == relation.T).all()\n",
    "\n",
    "\n",
    "def is_transitive(relation: np.ndarray) -> bool:\n",
    "    RoR = composition_RoR(relation)\n",
    "    Rp = union_two_relation(relation, RoR)\n",
    "    return (Rp == relation).all()\n",
    "\n",
    "\n",
    "def is_equivalece(relation: np.ndarray) -> bool:\n",
    "    return is_reflexive(relation) & is_symmetric(relation) & \\\n",
    "        is_transitive(relation)\n",
    "\n",
    "\n",
    "print('Is our train equivalence relation, equivalence?',\n",
    "    f'{is_equivalece(uni_train.equivalence_relation)}')\n",
    "\n",
    "print('Is our test equivalence relation, equivalence?',\n",
    "    f'{is_equivalece(uni_test.equivalence_relation)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Details \n",
    ">\n",
    "> For checking if our relation is equivalence we should check to\n",
    ">  be reflexive, symmetric, and transitive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "With our equivalence similarity relation, we can cluster our data\n",
    "by finding similarity classes and as our relation is equivalent\n",
    "if we find similarity class for sample x, we don't need to find\n",
    "the equivalent class for the other members of equivalence class x\n",
    "because as I proof before all of them has the same equivalence\n",
    "class as x so we'll cluster the equivalence class for x and then\n",
    "cluster rest of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster with alpha-cut 0.93 on train equivalence relation is\n",
      "([array([  0,   1,   3,   6,   8,  12,  13,  14,  15,  16,  17,  20,  22,\n",
      "        31,  41,  44,  45,  46,  47,  49,  52,  57,  58,  59,  60,  66,\n",
      "        67,  76,  78,  79,  80,  82,  85,  89,  98, 101, 105, 107, 112,\n",
      "       128, 142, 144, 146, 153, 154, 161, 164, 168, 171, 172, 173, 175,\n",
      "       176, 178, 180, 185, 186, 190, 195, 197, 200, 203, 210, 214, 222,\n",
      "       223, 224, 226, 227, 228, 229, 231, 232, 234, 236, 238, 242, 243,\n",
      "       244, 245, 250, 253, 255, 258, 263, 264, 265, 266, 267, 272, 275,\n",
      "       276, 279, 284, 285, 288, 290, 291, 293, 295, 297, 299, 302, 305,\n",
      "       312, 313, 314, 316, 317, 322, 324, 326, 327, 328, 330, 332, 333,\n",
      "       337, 340, 347, 349, 355, 359, 360, 361, 364, 367, 370, 371, 376,\n",
      "       381, 382, 383, 388, 395, 398, 413, 423, 427, 437, 439, 441, 442,\n",
      "       444, 445, 446, 448, 450, 452, 462, 463, 466, 469, 470, 480, 481,\n",
      "       489, 490]), array([  2,   4,   7,   9,  10,  11,  18,  19,  23,  25,  28,  29,  30,\n",
      "        32,  33,  34,  35,  39,  40,  43,  48,  50,  51,  53,  55,  56,\n",
      "        62,  63,  64,  65,  68,  73,  74,  75,  77,  81,  83,  84,  86,\n",
      "        87,  88,  90,  92,  93,  95,  99, 102, 104, 108, 109, 110, 113,\n",
      "       114, 115, 116, 118, 119, 120, 123, 124, 126, 127, 129, 130, 131,\n",
      "       132, 134, 135, 136, 138, 139, 140, 141, 143, 149, 150, 151, 155,\n",
      "       156, 157, 159, 162, 166, 167, 169, 170, 174, 177, 179, 181, 182,\n",
      "       187, 188, 189, 191, 193, 194, 198, 199, 201, 202, 204, 205, 206,\n",
      "       207, 211, 212, 213, 215, 216, 217, 220, 221, 225, 233, 235, 237,\n",
      "       240, 246, 248, 251, 252, 256, 257, 259, 260, 261, 262, 268, 269,\n",
      "       270, 271, 273, 274, 277, 278, 280, 281, 282, 283, 286, 287, 289,\n",
      "       292, 296, 298, 301, 303, 304, 308, 309, 310, 311, 315, 321, 325,\n",
      "       329, 331, 334, 335, 336, 338, 339, 342, 343, 344, 345, 346, 348,\n",
      "       350, 351, 352, 353, 356, 357, 358, 362, 363, 365, 366, 368, 369,\n",
      "       372, 374, 377, 378, 379, 384, 385, 387, 389, 391, 392, 393, 401,\n",
      "       402, 403, 405, 406, 407, 409, 411, 414, 415, 416, 418, 419, 420,\n",
      "       421, 422, 424, 425, 426, 428, 429, 430, 431, 432, 433, 434, 436,\n",
      "       438, 440, 447, 451, 453, 455, 456, 458, 459, 464, 465, 467, 468,\n",
      "       471, 473, 474, 475, 476, 484, 485, 491]), array([  5, 443]), array([21]), array([24, 36]), array([26]), array([27]), array([37]), array([38]), array([42]), array([54]), array([61]), array([69]), array([70]), array([71]), array([72]), array([91]), array([94]), array([96]), array([97]), array([100]), array([103]), array([106]), array([111]), array([117]), array([121]), array([122]), array([125]), array([133]), array([137]), array([145]), array([147]), array([148]), array([152]), array([158]), array([160]), array([163]), array([165]), array([183]), array([184]), array([192]), array([196]), array([208]), array([209]), array([218]), array([219]), array([230]), array([239]), array([241]), array([247]), array([249]), array([254]), array([294]), array([300]), array([306]), array([307, 318]), array([319]), array([320]), array([323]), array([341]), array([354]), array([373, 488]), array([375]), array([380]), array([386]), array([390]), array([394]), array([396]), array([397]), array([399]), array([400]), array([404]), array([408]), array([410]), array([412]), array([417]), array([435]), array([449]), array([454]), array([457]), array([460]), array([461]), array([472]), array([477]), array([478]), array([479]), array([482]), array([483]), array([486]), array([487])], 90)\n"
     ]
    }
   ],
   "source": [
    "def find_similarity_class(\n",
    "        universal: UniSet, target_sample: int, alpha: float) -> np.ndarray:\n",
    "    size_of_universal = universal.shape[0]\n",
    "    similarity_class = []\n",
    "    for sample in range(size_of_universal):\n",
    "        if(universal[sample, target_sample] >= alpha):\n",
    "            similarity_class.append(sample)\n",
    "    return np.array(similarity_class)\n",
    "\n",
    "\n",
    "def find_cluster(relation: np.ndarray, alpha: float, label=False):\n",
    "    size_of_universal = relation.shape[0]\n",
    "    classes = []\n",
    "    predicted_label = np.full((size_of_universal, 1), -1.0)\n",
    "    number_of_class = 0.0\n",
    "    for sample in range(size_of_universal):\n",
    "        if(predicted_label[sample] == -1):\n",
    "            new_class = find_similarity_class(relation, sample, alpha)\n",
    "            predicted_label[new_class] = number_of_class\n",
    "            number_of_class += 1\n",
    "            classes.append(new_class)\n",
    "    number_of_class = int(number_of_class)\n",
    "    if(label is True):\n",
    "        return predicted_label, number_of_class\n",
    "    return classes, number_of_class\n",
    "\n",
    "\n",
    "cluster_alpha_cut_93 = find_cluster(uni_train.equivalence_relation, 0.93)\n",
    "print('\\nCluster with alpha-cut 0.93 on train equivalence relation is')\n",
    "print(cluster_alpha_cut_93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Details \n",
    ">\n",
    "> ##### Function find_similarity_class\n",
    ">\n",
    "> Get a relation, sample, and $\\alpha$ then find similarity class \n",
    "> for that sample by selecting all samples that have similarity \n",
    "> $\\geqslant \\alpha$ with our sample and return a crisp set ([x])\n",
    ">\n",
    "> ##### Function find_cluster\n",
    ">\n",
    "> Get relation, $\\alpha$, and label then find clusters which in each\n",
    ">  cluster all pair of members have similarity $\\geqslant \\alpha$.\n",
    ">  first, all the elements are in cluster -1 (not clustered yet) and\n",
    ">  every time we select a member of cluster -1 and find its similarity\n",
    ">  class and create a new cluster with all elements of similarity class\n",
    ">  and remove them from cluster -1. cluster are labeled from 0 and if\n",
    ">  label=True our funcion retun vector predicted-label which is predicted\n",
    ">  label for all samples and if label=False return set of clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which $\\alpha$-cut clustering our data better\n",
    "\n",
    "I find all unique membership degrees in our equivalent relation then\n",
    "start from smallest to the biggest and check my accuracy of clustering\n",
    "with that membership degree if $\\nabla$ degree $\\geqslant \\varepsilon$\n",
    "(with the last degree that I checked) and select the most accurate degree\n",
    "as the best $\\alpha$-cut\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "I use **Confusion matrix** to find the label of clusters (argmax in each row)\n",
    "and then choose f1-score as accuracy metric because as $\\alpha$ increase\n",
    "precision increase and recall decrease and I want to find $\\alpha$ that satisfy both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha-cut is 0.8845327547116397 on train-dataset  with f1-score 0.9296903460837888\n"
     ]
    }
   ],
   "source": [
    "def evaluate(gold_label: np.ndarray, predict_label: np.ndarray,\n",
    "             method: str = 'f1-score') -> float:\n",
    "    diffrent_label_in_gold_label = np.unique(gold_label)\n",
    "    diffrent_label_in_predict_label = np.unique(predict_label)\n",
    "    confusion_matrix = np.array(\n",
    "        list(map(lambda k: list(map(\n",
    "            lambda s: sum((predict_label == k)*(gold_label == s))[0],\n",
    "            diffrent_label_in_gold_label)),\n",
    "            diffrent_label_in_predict_label)))\n",
    "    precision = np.sum(\n",
    "        np.max(confusion_matrix, axis=1)) / np.sum(confusion_matrix)\n",
    "    recall = np.sum(\n",
    "        np.max(confusion_matrix, axis=0)) / np.sum(confusion_matrix)\n",
    "    if(method == 'precision'):\n",
    "        return precision\n",
    "    if(method == 'recall'):\n",
    "        return recall\n",
    "    if(method == 'f1-score'):\n",
    "        return 2 * ((precision*recall)/(precision+recall))\n",
    "\n",
    "\n",
    "def find_best_alpha_cut(universal: UniSet, plotter: bool = False) -> float:\n",
    "    alpha_cut = []\n",
    "    accuracy = []\n",
    "    last_point = -1.0\n",
    "    for alpha in np.unique(universal.equivalence_relation):\n",
    "        if(alpha - last_point >= 0.001):\n",
    "            alpha_cut.append(alpha)\n",
    "            accuracy.append(evaluate(\n",
    "                universal.label,\n",
    "                find_cluster(universal.equivalence_relation, alpha, True)[0]))\n",
    "            last_point = alpha\n",
    "    \n",
    "    if(plotter is True):\n",
    "        plt.plot(alpha_cut, accuracy, label=\"f1-score\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.xlabel('α')\n",
    "        plt.ylabel('Accuracy (f1-score)')\n",
    "        plt.title(f'Plot accuracy of clustering with diffrent alpha')\n",
    "        plt.show()\n",
    "    \n",
    "    return alpha_cut[np.argmax(accuracy)], np.max(accuracy)\n",
    "\n",
    "\n",
    "best_alpha_cut, best_alpha_cut_accuracy = find_best_alpha_cut(uni_train, True)\n",
    "print(f'Best alpha-cut is {best_alpha_cut} on train-dataset',\n",
    "        f' with f1-score {best_alpha_cut_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Details \n",
    "> \n",
    "> ##### Confusion matrix\n",
    "> \n",
    "> <p align=\"center\">\n",
    ">  <img src=\"report/confusion-matrix.png\">\n",
    "> </p>\n",
    "> \n",
    "> Its $(K * S)$ matrix that $a_{k, s}=$ total number of samples clustered\n",
    ">  to the kᵗʰ cluster and belongs to the sᵗʰ class.\n",
    ">  $$\\text { Precision }=\\frac{\\sum_{k}\n",
    "\\max _{s}\\left\\{a_{k s}\\right\\}}{\\sum_{k} \\sum_{s} a_{k s}}$$\n",
    ">  $$\\operatorname{Recall}=\\frac{\\sum_{s}\n",
    "\\max _{k}\\left\\{a_{k s}\\right\\}}{\\left(\\sum_{k} \\sum_{s} a_{k s}+U\\right)}$$\n",
    ">  $$F1-score=2 \\times \\frac{\\text\n",
    "{ Precision } \\times \\text { Recall }}{\\text { Precision }+\\text { Recall }}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test our $\\alpha$\n",
    "\n",
    "To do that we use our test set, find relation on the test set and make it transitive to be equivalence relation and then find our clusters with $\\alpha$-cut than we find in the training phase the compute test accuracy to see how well our $\\alpha$-cut works on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our f1-score on test-dataset with best alpha-cut on train-set is 0.9396551724137931\n"
     ]
    }
   ],
   "source": [
    "predicted_label_test = find_cluster(\n",
    "    uni_test.equivalence_relation, best_alpha_cut, True)[1]\n",
    "test_accuracy = evaluate(uni_test.label, predicted_label_test)\n",
    "print('Our f1-score on test-dataset with best alpha-cut on train-set is',\n",
    "        test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Thanks for your time</center>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
